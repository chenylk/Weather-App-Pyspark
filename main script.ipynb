{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "from config import API_KEY\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, DateType, DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, col, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: None\n",
      "PATH: c:\\Users\\lucas\\anaconda3\\envs\\pyspark_env;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV\\Library\\mingw-w64\\bin;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV\\Library\\usr\\bin;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV\\Library\\bin;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV\\Scripts;C:\\Users\\lucas\\anaconda3\\envs\\PYSPARK_ENV\\bin;C:\\Users\\lucas\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp;C:\\Program Files\\Rockwell Software\\RSCommon;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Git\\cmd;C:\\Program Files\\MongoDB\\Server\\4.4\\bin;C:\\Program Files\\dotnet;C:\\Program Files\\PuTTY;C:\\Program Files\\Amazon\\AWSCLIV2;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\130\\Tools\\Binn;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\Tools\\Binn;C:\\Program Files\\Microsoft SQL Server\\140\\Tools\\Binn;C:\\Program Files\\Microsoft SQL Server\\140\\DTS\\Binn;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\Docker\\Docker\\resources\\bin;C:\\Program Files\\MATLAB\\R2023a\\bin;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2023.1.1;C:\\Users\\lucas\\AppData\\Roaming\\TinyTeX\\bin\\win32;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin;C:\\Users\\lucas\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\lucas\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\lucas\\.dotnet\\tools;C:\\Users\\lucas\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin;.\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-1.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"PATH: {os.environ.get('PATH')}\")\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-1.8'\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "\n",
    "#os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\lucas\\anaconda3\\python.exe\"\n",
    "#os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\lucas\\anaconda3\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"weatherapp\") \\\n",
    "    .getOrCreate()\n",
    "    #.config(\"spark.driver.memory\", \"4g\") \\\n",
    "    #.config(\"spark.executor.memory\", \"4g\") \\\n",
    "    #.config(\"spark.python.worker.memory\", \"4g\") \\\n",
    "    #.config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    #.config(\"spark.pyspark.python\", r\"C:\\Users\\lucas\\anaconda3\\python.exe\") \\\n",
    "    #.config(\"spark.pyspark.driver.python\", r\"C:\\Users\\lucas\\anaconda3\\python.exe\") \\\n",
    "#spark.sparkContext.addFile(r\"C:\\Users\\lucas\\anaconda3\\python.exe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://api.weatherapi.com/v1/current.json\"\n",
    "#http://api.weatherapi.com/v1/history.json\n",
    "api_key = API_KEY\n",
    "city = 'Philadelphia'\n",
    "end_date = datetime.now()\n",
    "start_date = datetime.now() - timedelta(days=90)\n",
    "start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "params = {\n",
    "    'key': api_key,\n",
    "    'q': city\n",
    "}\n",
    "\n",
    "response = requests.get(url,params=params)\n",
    "data = response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_updated_epoch': 1732400100,\n",
       " 'last_updated': '2024-11-23 17:15',\n",
       " 'temp_c': 10.3,\n",
       " 'temp_f': 50.5,\n",
       " 'is_day': 0,\n",
       " 'condition': {'text': 'Partly cloudy',\n",
       "  'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png',\n",
       "  'code': 1003},\n",
       " 'wind_mph': 13.2,\n",
       " 'wind_kph': 21.2,\n",
       " 'wind_degree': 292,\n",
       " 'wind_dir': 'WNW',\n",
       " 'pressure_mb': 1008.0,\n",
       " 'pressure_in': 29.75,\n",
       " 'precip_mm': 0.0,\n",
       " 'precip_in': 0.0,\n",
       " 'humidity': 48,\n",
       " 'cloud': 50,\n",
       " 'feelslike_c': 7.6,\n",
       " 'feelslike_f': 45.8,\n",
       " 'windchill_c': 8.0,\n",
       " 'windchill_f': 46.4,\n",
       " 'heatindex_c': 10.7,\n",
       " 'heatindex_f': 51.2,\n",
       " 'dewpoint_c': 4.5,\n",
       " 'dewpoint_f': 40.1,\n",
       " 'vis_km': 16.0,\n",
       " 'vis_miles': 9.0,\n",
       " 'uv': 0.0,\n",
       " 'gust_mph': 17.9,\n",
       " 'gust_kph': 28.8}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['current']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history\n",
    "def loop_date(start_date,end_date):\n",
    "    date_list = []\n",
    "    while start_date <= end_date:\n",
    "        date_list.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "        start_date += timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def get_city_attr(cities,start_date,end_date):\n",
    "    url = \"http://api.weatherapi.com/v1/history.json\"\n",
    "    api_key = API_KEY\n",
    "    date_list = loop_date(start_date,end_date)\n",
    "    measure_metrics = ['avgtemp_f','maxwind_mph','totalprecip_in','condition']\n",
    "    spark_values_store = []\n",
    "    for city in cities:\n",
    "        for date in date_list:\n",
    "            params = {\n",
    "                'key': api_key,\n",
    "                'q': city,\n",
    "                'dt': date\n",
    "            }\n",
    "            response = requests.get(url,params=params)\n",
    "            data = response.json()\n",
    "            date_store = {'date':date, 'city':city}\n",
    "            for metric in measure_metrics:\n",
    "                date_store[metric] = data['forecast']['forecastday'][0]['day'][metric]\n",
    "            spark_values_store.append(date_store)\n",
    "    return spark_values_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_attr(cities,unit ='F'):\n",
    "    url = \"http://api.weatherapi.com/v1/current.json\"\n",
    "    api_key = API_KEY\n",
    "    measure_metrics = ['name','temp_f','humidity','condition']\n",
    "    spark_values_store = []\n",
    "    for city in cities:\n",
    "        params = {\n",
    "            'key': api_key,\n",
    "            'q': city\n",
    "        }\n",
    "        response = requests.get(url,params=params)\n",
    "        data = response.json()\n",
    "        data_store = {}\n",
    "        for metric in measure_metrics:\n",
    "            if metric == 'name':\n",
    "                data_store[metric] = data['location'][metric]\n",
    "                continue\n",
    "            if metric == 'condition':\n",
    "                data_store[metric] = data['current'][metric]['text']\n",
    "                continue\n",
    "            data_store[metric] = data['current'][metric]\n",
    "        if unit == 'C':\n",
    "            data_store['temp_c'] = (data_store['temp_f'] - 32) * (5/9)\n",
    "            del data_store['temp_f']\n",
    "        spark_values_store.append(data_store)\n",
    "    return spark_values_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Philadelphia','Atlanta','Denver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\",\n",
    "    \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\",\n",
    "    \"London\", \"Paris\", \"Berlin\", \"Madrid\", \"Rome\",\n",
    "    \"Tokyo\", \"Beijing\", \"Mumbai\", \"Sydney\", \"Cape Town\",\n",
    "    \"Dubai\", \"Singapore\", \"Seoul\", \"Moscow\", \"Rio de Janeiro\",\n",
    "    \"Buenos Aires\", \"Cairo\", \"Bangkok\", \"Istanbul\", \"Toronto\",\n",
    "    \"Mexico City\", \"Jakarta\", \"Kuala Lumpur\", \"Lagos\", \"Nairobi\",\n",
    "    \"Johannesburg\", \"Melbourne\", \"Lima\", \"Bogotá\", \"Santiago\",\n",
    "    \"Amsterdam\", \"Brussels\", \"Warsaw\", \"Vienna\", \"Stockholm\",\n",
    "    \"Oslo\", \"Copenhagen\", \"Helsinki\", \"Prague\", \"Zurich\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.now()\n",
    "start_date = datetime.now() - timedelta(days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = get_city_attr(cities,start_date,end_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = get_city_attr(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('avgtemp_f', FloatType(), True),\n",
    "    StructField('maxwind_mph', FloatType(), True),\n",
    "    StructField('totalprecip_in', FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_record(record):\n",
    "    return {\n",
    "        'date' : str(record['date']),\n",
    "        'city' : str(record['city']),\n",
    "        'avgtemp_f' : double(record['avgtemp_f']),\n",
    "        'maxwind_mph' : double(record['maxwind_mph']),\n",
    "        'totalprecip_in' : float(record['totalprecip_in'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the city with the highest temperature\n",
    "window_spec = Window.partitionBy().orderBy(col('temp_f').desc())\n",
    "df_with_rank = weather_df.withColumn('rank',rank().over(window_spec))\n",
    "city_with_highest_temp = df_with_rank.filter(col('rank')==1).select('name').collect()[0][0]\n",
    "print(city_with_highest_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jakarta'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def city_compare(city1,city2,df):\n",
    "    if \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------------+------+----+\n",
      "|    condition|humidity|          name|temp_f|rank|\n",
      "+-------------+--------+--------------+------+----+\n",
      "|         Mist|      63|       Jakarta|  89.8|   1|\n",
      "|Partly cloudy|      70|     Singapore|  86.5|   2|\n",
      "|Partly cloudy|      66|       Bangkok|  86.4|   3|\n",
      "|Partly cloudy|      79|  Kuala Lumpur|  84.6|   4|\n",
      "|        Clear|      94|         Lagos|  79.3|   5|\n",
      "|        Sunny|      65|        Sydney|  78.1|   6|\n",
      "|        Sunny|      69|         Dubai|  77.4|   7|\n",
      "|Partly cloudy|      65|     Melbourne|  77.0|   8|\n",
      "|     Overcast|      44|        Mumbai|  75.4|   9|\n",
      "|     Overcast|      78|  Buenos Aires|  72.0|  10|\n",
      "|        Clear|      73|Rio De Janeiro|  72.0|  10|\n",
      "|Partly cloudy|      21|       Phoenix|  71.1|  12|\n",
      "|Partly Cloudy|      82|          Lima|  65.7|  13|\n",
      "|        Clear|      47|   San Antonio|  64.9|  14|\n",
      "|     Overcast|      64|       Houston|  64.4|  15|\n",
      "|Partly cloudy|      60|     San Diego|  63.0|  16|\n",
      "|        Clear|      17|   Mexico City|  62.8|  17|\n",
      "|Partly cloudy|      94|       Nairobi|  61.3|  18|\n",
      "|Partly cloudy|      68|   Los Angeles|  61.2|  19|\n",
      "|        Clear|      63|  Johannesburg|  61.2|  19|\n",
      "+-------------+--------+--------------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
